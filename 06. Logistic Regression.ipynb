{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prepare Data  \n",
    "   <ol type=\"i\">\n",
    "      <li>Data Loader</li>\n",
    "      <li>Normalizing Data</li>\n",
    "   </ol>  \n",
    "\n",
    "2. Designing the model\n",
    "   <ol type=\"i\">\n",
    "      <li>input size</li>\n",
    "      <li>output size</li>\n",
    "      <li>forward pass</li>\n",
    "   </ol>  \n",
    "   \n",
    "3. Construct loss and optimizer  \n",
    "\n",
    "4. Training Loop\n",
    "   <ol type=\"i\">\n",
    "      <li>forward pass: compute prediction and loss</li>\n",
    "      <li>backward pass: gradients</li>\n",
    "      <li>update weights</li>\n",
    "   </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Loading Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "# Casting to float32 tensor. Otherwise inferencing stage model(X) runs into a dtype error. \n",
    "# Scaling using sklearn.preprocessing.StandardScalar.\n",
    "X_train_unscaled, X_test_unscaled, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "n_samples, n_features = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii. Scaling the Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mean and std. is found only for train data. For test data same mean and std. is used from train set.\n",
    "- the output features are untouched in preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = StandardScaler().fit_transform(X_train_unscaled)\n",
    "X_test = StandardScaler().fit(X_train_unscaled).transform(X_test_unscaled)\n",
    "\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train_unscaled)\n",
    "# X_test = sc.transform(X_test_unscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii. Casting to float32 tensors adn Trasforming y Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casting to float32 tensor. Otherwise inferencing stage model(X) runs into a dtype error. \n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(-1, 1)\n",
    "y_test = y_test.view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Desining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One layer neural network\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)    #This applies a linear trasformation \n",
    "        #While initializing the weights for the matrix as well. Contains learnable parameters.\n",
    "        #No activation function.\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "    \n",
    "model = Model(n_features)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loss Function and the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "Error = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #Forward pass and loss calculation\n",
    "    y_pred = model(X_train)\n",
    "    loss = Error(y_pred, y_train)\n",
    "\n",
    "    #Backward pass and weight update\n",
    "    loss.backward()\n",
    "\n",
    "    #House keeping. Gradient accumilation should be\n",
    "    #restarted befor new iterations\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(f'epoch:{epoch+1}, loss: {loss.item():4f}')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluations should be done on test set without the gradients being tracked.\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    \n",
    "    #print(type(y_predicted))\n",
    "    #print(y_predicted) # This is not rounded to the class. Since it is a sigmoid output.\n",
    "    #Round to take the nearest class.\n",
    "\n",
    "    y_preicted_class = y_predicted.round()\n",
    "    \n",
    "    #This is how much of y_predicted_class are equal to y_test shown as a percentage.\n",
    "    accuracy = y_preicted_class.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy: {accuracy.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foot Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. sklearn.preprocessing.StandardScaler\n",
    "\n",
    "- StandardScaler removes the mean and scales the data to unit variance. The outliers have an influence when computing the empirical mean and standard deviation. StandardScaler therefore cannot guarantee balanced feature scales in the presence of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot X_train without scaling\n",
    "axs[0, 0].scatter(range(len(X_train_unscaled)), X_train_unscaled[:, 0], color='blue', label='X_train')\n",
    "axs[0, 0].set_title('X_train without Standard Scaling')\n",
    "axs[0, 0].set_xlabel('Index')\n",
    "axs[0, 0].set_ylabel('Feature 1')\n",
    "\n",
    "# Plot X_train with scaling\n",
    "axs[0, 1].scatter(range(len(X_train)), X_train[:, 0], color='red', label='X_train Scaled')\n",
    "axs[0, 1].set_title('X_train with Standard Scaling')\n",
    "axs[0, 1].set_xlabel('Index')\n",
    "axs[0, 1].set_ylabel('Feature 1 (Scaled)')\n",
    "\n",
    "# Plot X_test without scaling\n",
    "axs[1, 0].scatter(range(len(X_test_unscaled)), X_test_unscaled[:, 0], color='blue', label='X_test')\n",
    "axs[1, 0].set_title('X_test without Standard Scaling')\n",
    "axs[1, 0].set_xlabel('Index')\n",
    "axs[1, 0].set_ylabel('Feature 1')\n",
    "\n",
    "# Plot X_test with scaling\n",
    "axs[1, 1].scatter(range(len(X_test)), X_test[:, 0], color='red', label='X_test Scaled')\n",
    "axs[1, 1].set_title('X_test with Standard Scaling')\n",
    "axs[1, 1].set_xlabel('Index')\n",
    "axs[1, 1].set_ylabel('Feature 1 (Scaled)')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sklearn.preprocessing.StandardScaler.fit_transform\n",
    "  - The fit method is calculating the mean and variance of each of the features present in our data. The transform method is transforming all the features using the respective mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sklearn.preprocessing.StandardScaler.transform\n",
    "  - Using the transform method we can use the same mean and variance as it is calculated from our training data to transform our test data. Thus, the parameters learned by our model using the training data will help us to transform our test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why we use .fit_trasform on train data and .trasform on test data\n",
    "  - If we will use the fit method on our test data too, we will compute a new mean and variance that is a new scale for each feature and will let our model learn about our test data too. Thus, what we want to keep as a surprise is no longer unknown to our model and we will not get a good estimate of how our model is performing on the test (unseen) data which is the ultimate goal of building a model using machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. breast cancer wisconsin dataset\n",
    "\n",
    "sklearn.datasets provides few toy datasets. This is a dataset on a binary classification with 569 datapoints with 30 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. sklearn.model_selection.train_test_split\n",
    "\n",
    "- train/test split ratio is taken around 80%, 20% for smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unscaled, X_test_unscaled, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Problem \n",
    "  -  if x_n is either 0 or 1, one of the log terms would be mathematically undefined in Cross Entropy Loss. \n",
    "  - if either y = 0 or y =1, then we would be multiplying 0 with infinity. \n",
    "  -  we would also have an infinite term in our gradient which  would make BCELoss’s backward method nonlinear with respect to x_n\n",
    "- Answer \n",
    "  - BCELoss clamps its log function outputs to be greater than or equal to -100. This way, we can always have a finite loss value and a linear backward method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
