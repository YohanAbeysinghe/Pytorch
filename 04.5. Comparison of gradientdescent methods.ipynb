{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Full Automatic Using <code>model=nn.Linear()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = -4.611\n",
      "epoch 1: w = -0.468, loss = 64.46286011\n",
      "epoch 5: w = 0.670, loss = 15.02735329\n",
      "epoch 9: w = 1.220, loss = 3.56565261\n",
      "epoch 13: w = 1.487, loss = 0.90675443\n",
      "epoch 17: w = 1.616, loss = 0.28849059\n",
      "epoch 21: w = 1.681, loss = 0.14331460\n",
      "epoch 25: w = 1.713, loss = 0.10784941\n",
      "epoch 29: w = 1.730, loss = 0.09785925\n",
      "epoch 33: w = 1.740, loss = 0.09381659\n",
      "epoch 37: w = 1.747, loss = 0.09119352\n",
      "Prediction before training: f(5) = 9.479\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "T = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "#Defining the model by taking number of features and number of samples\n",
    "#No need to intialize the weights manually\n",
    "n_samples, n_features = X.shape\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "#Inferencing variable should also be a tensor to be inputed to model=nn.Linear()\n",
    "x_test = torch.tensor([5], dtype=torch.float32)\n",
    "print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "learning_rate = 0.01 #Increase this and you can see gradient explosion.\n",
    "n_iters = 40\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "#Error/Loss and the optimizer where weight updates happen is defined using torch.\n",
    "#optimizer automatically updates the weights after one training loop iteration.\n",
    "#No need to do it manually and also to stop autograd from tracking weight update.\n",
    "Error = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) #Instead of simply giving w.\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    Y = model(X)\n",
    "\n",
    "    # loss is done using torch.MOSE\n",
    "    E = Error(T, Y)\n",
    "    \n",
    "    # Backward pass is done automatically now.\n",
    "    E.backward()\n",
    "\n",
    "    # Weight update is done automatically using optimizer. No need to wrap things in \n",
    "    # torch.no_grad()\n",
    "    optimizer.step()\n",
    "\n",
    "    # The w.grad where the gradients are accumilated should be turned zero before next iteration.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 4 == 0:\n",
    "\n",
    "        #Weights are inserted into model.parameters(). Need to unroll them.\n",
    "        [w, b] = model.parameters()\n",
    "\n",
    "        # print(\"w: \", w)\n",
    "        # print(\"w[0]: \", w[0])\n",
    "        # print(\"w[0][0]]: \", w[0][0])\n",
    "        # w is a tensor, that is a list of a list. Hence w[0][0] is needed.\n",
    "\n",
    "        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {E:.8f}')\n",
    "     \n",
    "#--------------------------------------------------------------------------\n",
    "print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Loss and Optimizer from Torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 5: w = 1.113, loss = 8.17471695\n",
      "epoch 9: w = 1.537, loss = 2.22753215\n",
      "epoch 13: w = 1.758, loss = 0.60698116\n",
      "epoch 17: w = 1.874, loss = 0.16539653\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 25: w = 1.966, loss = 0.01228084\n",
      "epoch 29: w = 1.982, loss = 0.00334642\n",
      "epoch 33: w = 1.991, loss = 0.00091188\n",
      "epoch 37: w = 1.995, loss = 0.00024848\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 45: w = 1.999, loss = 0.00001845\n",
      "epoch 49: w = 1.999, loss = 0.00000503\n",
      "Prediction after training: f(5) = 9.997\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "T = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "# Intializing the weights. This is the variable we are calculating the gradient on.\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True) \n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "#forward function where the model get implicitly defined.\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "#Inferencing variable.\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "learning_rate = 0.01 #Increase this and you can see gradient explosion.\n",
    "n_iters = 50\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "#Error/Loss and the optimizer where weight updates happen is defined using torch.\n",
    "#optimizer automatically updates the weights after one training loop iteration.\n",
    "#No need to do it manually and also to stop autograd from tracking weight update.\n",
    "Error = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    Y = forward(X)\n",
    "\n",
    "    # loss is done using torch.nn.MSELoss()\n",
    "    E = Error(T, Y)\n",
    "    \n",
    "    # Backward pass is done automatically now.\n",
    "    E.backward()\n",
    "\n",
    "    # Weight update is done automatically using optimizer. No need to wrap things in \n",
    "    # torch.no_grad()\n",
    "    optimizer.step()\n",
    "\n",
    "    # The w.grad where the gradients are accumilated should be turned zero before next iteration.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 4 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {E:.8f}')\n",
    "     \n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using AutoGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 5: w = 1.113, loss = 8.17471695\n",
      "epoch 9: w = 1.537, loss = 2.22753215\n",
      "epoch 13: w = 1.758, loss = 0.60698116\n",
      "epoch 17: w = 1.874, loss = 0.16539653\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 25: w = 1.966, loss = 0.01228084\n",
      "epoch 29: w = 1.982, loss = 0.00334642\n",
      "epoch 33: w = 1.991, loss = 0.00091188\n",
      "epoch 37: w = 1.995, loss = 0.00024848\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 45: w = 1.999, loss = 0.00001845\n",
      "epoch 49: w = 1.999, loss = 0.00000503\n",
      "Prediction after training: f(5) = 9.997\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "T = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "# Intializing the weights. This is the variable we are calculating the gradient on.\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True) \n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "#forward function where the model get implicitly defined.\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "#Inferencing variable.\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "learning_rate = 0.01 #Increase this and you can see gradient explosion.\n",
    "n_iters = 50\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "#Loss function is defined by hand. \n",
    "def loss(y, t): #y=prediction, t=target.\n",
    "    return ((y-t)**2).mean()\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    Y = forward(X)\n",
    "\n",
    "    # loss\n",
    "    E = loss(T, Y)\n",
    "    \n",
    "    # Backward pass is done automatically now.\n",
    "    E.backward()\n",
    "\n",
    "    # updating weights should not be tracke from the autograd module. This should be done manually.\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "    # The w.grad where the gradients are accumilated should be turned zero before next iteration.\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch % 4 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {E:.8f}')\n",
    "     \n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Done Everything Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 5: w = 1.113, loss = 8.17471600\n",
      "epoch 9: w = 1.537, loss = 2.22753215\n",
      "epoch 13: w = 1.758, loss = 0.60698175\n",
      "epoch 17: w = 1.874, loss = 0.16539653\n",
      "epoch 21: w = 1.934, loss = 0.04506905\n",
      "epoch 25: w = 1.966, loss = 0.01228092\n",
      "epoch 29: w = 1.982, loss = 0.00334642\n",
      "epoch 33: w = 1.991, loss = 0.00091188\n",
      "epoch 37: w = 1.995, loss = 0.00024848\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 45: w = 1.999, loss = 0.00001845\n",
      "epoch 49: w = 1.999, loss = 0.00000503\n",
      "Prediction after training: f(5) = 9.997\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "T = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "w = 0.0\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "learning_rate = 0.01 #Increase this and you can see gradient explosion.\n",
    "n_iters = 50\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "def loss(y, t): #y=prediction, t=target.\n",
    "    return ((y-t)**2).mean()\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "def gradient(x, t, y):\n",
    "    return np.mean(2*x*(y - t))\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    Y = forward(X)\n",
    "\n",
    "    # loss\n",
    "    E = loss(T, Y)\n",
    "    \n",
    "    # calculate gradients\n",
    "    dw = gradient(X, T, Y)\n",
    "\n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    if epoch % 4 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {E:.8f}')\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------  \n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
